"""
G√©n√©rateur d'embeddings pour chunks de texte
Optimis√© pour le fran√ßais juridique/RH
"""

import logging
import time
from typing import List, Dict, Optional, Tuple
import numpy as np
from sentence_transformers import SentenceTransformer

from ..data_processing.text_processor import TextChunk

class EmbeddingsGenerator:
    """
    G√©n√©rateur d'embeddings sp√©cialis√© pour textes juridiques fran√ßais
    
    Analogie : Traducteur qui convertit des phrases fran√ßaises 
    en "codes num√©riques" que l'ordinateur comprend
    """
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        Initialise le g√©n√©rateur d'embeddings
        
        Args:
            model_name: Nom du mod√®le √† utiliser
                       - all-MiniLM-L6-v2: Rapide, 384 dims, bon pour usage g√©n√©ral
                       - paraphrase-multilingual: Meilleur pour fran√ßais mais plus lourd
        """
        self.model_name = model_name
        self.model = None
        self.embedding_dimension = None
        
        # Configuration logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # Statistiques de performance
        self.stats = {
            'total_embeddings_generated': 0,
            'total_processing_time': 0.0,
            'average_time_per_embedding': 0.0,
            'model_loaded_at': None
        }
        
        # Configuration pour optimisation batch
        self.batch_size = 32  # Traiter par lots pour performance
        
    def load_model(self) -> bool:
        """
        Charge le mod√®le d'embeddings
        
        Analogie : Comme charger un dictionnaire sp√©cialis√© 
        dans la m√©moire avant de commencer √† traduire
        
        Returns:
            True si chargement r√©ussi, False sinon
        """
        try:
            self.logger.info(f"üîÑ Chargement du mod√®le {self.model_name}...")
            start_time = time.time()
            
            # Charger le mod√®le sentence-transformers
            self.model = SentenceTransformer(self.model_name)
            
            # D√©terminer la dimension des embeddings
            test_embedding = self.model.encode(["test"])
            self.embedding_dimension = test_embedding.shape[1]
            
            load_time = time.time() - start_time
            self.stats['model_loaded_at'] = time.strftime('%Y-%m-%d %H:%M:%S')
            
            self.logger.info(f"‚úÖ Mod√®le charg√© en {load_time:.2f}s")
            self.logger.info(f"üìä Dimension embeddings: {self.embedding_dimension}")
            self.logger.info(f"üéØ Mod√®le optimis√© pour: {self._get_model_info()}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur chargement mod√®le: {e}")
            return False
    
    def _get_model_info(self) -> str:
        """Retourne des infos sur le mod√®le utilis√©"""
        model_infos = {
            "all-MiniLM-L6-v2": "Usage g√©n√©ral, rapide, 384 dimensions",
            "paraphrase-multilingual-MiniLM-L12-v2": "Multilingue, fran√ßais optimis√©",
            "distiluse-base-multilingual-cased": "Multilingue, bonne qualit√©"
        }
        
        for model_key, info in model_infos.items():
            if model_key in self.model_name:
                return info
        
        return "Mod√®le personnalis√©"
    
    def preprocess_text_for_embedding(self, text: str) -> str:
        """
        Pr√©processe le texte avant g√©n√©ration embedding
        
        Analogie : Comme nettoyer et formater un texte 
        avant de le soumettre √† un traducteur
        
        Args:
            text: Texte brut √† pr√©processer
            
        Returns:
            Texte nettoy√© optimis√© pour embedding
        """
        if not text:
            return ""
        
        # Nettoyer le texte
        cleaned = text.strip()
        
        # Limiter la longueur (mod√®les ont des limites)
        max_length = 512  # Tokens approximatifs
        if len(cleaned) > max_length:
            # Couper intelligemment (√©viter de couper au milieu d'un mot)
            cleaned = cleaned[:max_length]
            last_space = cleaned.rfind(' ')
            if last_space > max_length * 0.8:  # Si l'espace est assez proche
                cleaned = cleaned[:last_space]
            
            self.logger.warning(f"‚ö†Ô∏è Texte tronqu√© √† {len(cleaned)} caract√®res")
        
        return cleaned
    
    def generate_single_embedding(self, text: str) -> Optional[np.ndarray]:
        """
        G√©n√®re un embedding pour un texte unique
        
        Args:
            text: Texte √† encoder
            
        Returns:
            Array numpy de l'embedding ou None si erreur
        """
        if not self.model:
            self.logger.error("‚ùå Mod√®le non charg√©")
            return None
        
        if not text or len(text.strip()) < 3:
            self.logger.warning("‚ö†Ô∏è Texte trop court pour embedding")
            return None
        
        try:
            start_time = time.time()
            
            # Pr√©processer
            clean_text = self.preprocess_text_for_embedding(text)
            
            # G√©n√©rer embedding
            embedding = self.model.encode([clean_text])
            
            # Extraire le vecteur (premi√®re dimension du batch)
            embedding_vector = embedding[0]
            
            # Statistiques
            processing_time = time.time() - start_time
            self.stats['total_embeddings_generated'] += 1
            self.stats['total_processing_time'] += processing_time
            self.stats['average_time_per_embedding'] = (
                self.stats['total_processing_time'] / 
                self.stats['total_embeddings_generated']
            )
            
            self.logger.debug(f"‚úÖ Embedding g√©n√©r√© en {processing_time:.3f}s")
            
            return embedding_vector
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration embedding: {e}")
            return None
    
    def generate_batch_embeddings(self, texts: List[str]) -> List[Optional[np.ndarray]]:
        """
        G√©n√®re des embeddings en lot (plus efficace)
        
        Analogie : Traiter plusieurs phrases d'un coup 
        plut√¥t qu'une par une (plus rapide)
        
        Args:
            texts: Liste des textes √† encoder
            
        Returns:
            Liste des embeddings (None si erreur sur un texte)
        """
        if not self.model:
            self.logger.error("‚ùå Mod√®le non charg√©")
            return [None] * len(texts)
        
        if not texts:
            return []
        
        self.logger.info(f"üîÑ G√©n√©ration embeddings batch: {len(texts)} textes")
        
        # Pr√©processer tous les textes
        clean_texts = []
        valid_indices = []
        
        for i, text in enumerate(texts):
            if text and len(text.strip()) >= 3:
                clean_texts.append(self.preprocess_text_for_embedding(text))
                valid_indices.append(i)
            else:
                self.logger.warning(f"‚ö†Ô∏è Texte {i} ignor√© (trop court)")
        
        if not clean_texts:
            self.logger.warning("‚ö†Ô∏è Aucun texte valide pour embedding")
            return [None] * len(texts)
        
        try:
            start_time = time.time()
            
            # G√©n√©rer embeddings en batch
            embeddings = self.model.encode(clean_texts, batch_size=self.batch_size)
            
            # Reconstituer la liste compl√®te avec None pour textes invalides
            result_embeddings = [None] * len(texts)
            for i, valid_idx in enumerate(valid_indices):
                result_embeddings[valid_idx] = embeddings[i]
            
            # Statistiques
            processing_time = time.time() - start_time
            valid_count = len(clean_texts)
            
            self.stats['total_embeddings_generated'] += valid_count
            self.stats['total_processing_time'] += processing_time
            self.stats['average_time_per_embedding'] = (
                self.stats['total_processing_time'] / 
                self.stats['total_embeddings_generated']
            )
            
            self.logger.info(f"‚úÖ Batch termin√©: {valid_count}/{len(texts)} embeddings en {processing_time:.2f}s")
            self.logger.info(f"üìä Performance: {valid_count/processing_time:.1f} embeddings/seconde")
            
            return result_embeddings
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur batch embeddings: {e}")
            return [None] * len(texts)
    
    def process_text_chunks(self, chunks: List[TextChunk]) -> List[Tuple[TextChunk, Optional[np.ndarray]]]:
        """
        Traite une liste de chunks et g√©n√®re leurs embeddings
        
        M√©thode principale pour notre pipeline
        
        Args:
            chunks: Liste des chunks √† traiter
            
        Returns:
            Liste de tuples (chunk, embedding)
        """
        if not chunks:
            return []
        
        self.logger.info(f"üéØ Traitement de {len(chunks)} chunks de texte")
        
        # Extraire les textes des chunks
        texts = [chunk.text for chunk in chunks]
        
        # G√©n√©rer embeddings en batch
        embeddings = self.generate_batch_embeddings(texts)
        
        # Combiner chunks et embeddings
        results = []
        successful_embeddings = 0
        
        for chunk, embedding in zip(chunks, embeddings):
            results.append((chunk, embedding))
            if embedding is not None:
                successful_embeddings += 1
        
        success_rate = (successful_embeddings / len(chunks)) * 100
        self.logger.info(f"‚úÖ Traitement termin√©: {successful_embeddings}/{len(chunks)} embeddings g√©n√©r√©s ({success_rate:.1f}%)")
        
        return results
    
    def get_embedding_stats(self) -> Dict:
        """
        Retourne les statistiques de performance
        
        Returns:
            Dictionnaire avec les m√©triques de performance
        """
        return {
            **self.stats,
            'model_name': self.model_name,
            'embedding_dimension': self.embedding_dimension,
            'batch_size': self.batch_size,
            'model_loaded': self.model is not None
        }
    
    def test_embedding_quality(self, test_texts: List[str] = None) -> Dict:
        """
        Test de qualit√© des embeddings avec exemples
        
        Args:
            test_texts: Textes de test (utilise exemples par d√©faut si None)
            
        Returns:
            R√©sultats des tests de similarit√©
        """
        if test_texts is None:
            # Exemples de test pour le domaine juridique/RH fran√ßais
            test_texts = [
                "cr√©ation d'une SARL",
                "cr√©er une soci√©t√© √† responsabilit√© limit√©e",
                "licenciement √©conomique",
                "formation professionnelle",
                "cotisations URSSAF",
                "charges sociales entreprise"
            ]
        
        if not self.model:
            return {"error": "Mod√®le non charg√©"}
        
        self.logger.info("üß™ Test qualit√© embeddings...")
        
        # G√©n√©rer embeddings pour tests
        embeddings = self.generate_batch_embeddings(test_texts)
        valid_embeddings = [(i, emb) for i, emb in enumerate(embeddings) if emb is not None]
        
        if len(valid_embeddings) < 2:
            return {"error": "Pas assez d'embeddings valides pour test"}
        
        # Calculer similarit√©s
        similarities = []
        
        for i, (idx1, emb1) in enumerate(valid_embeddings):
            for j, (idx2, emb2) in enumerate(valid_embeddings[i+1:], i+1):
                # Similarit√© cosine
                similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
                similarities.append({
                    'text1': test_texts[idx1],
                    'text2': test_texts[idx2], 
                    'similarity': float(similarity)
                })
        
        # Trier par similarit√© d√©croissante
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return {
            'test_texts_count': len(test_texts),
            'valid_embeddings': len(valid_embeddings),
            'top_similarities': similarities[:3],  # Top 3
            'average_similarity': np.mean([s['similarity'] for s in similarities]),
            'embedding_dimension': self.embedding_dimension
        }