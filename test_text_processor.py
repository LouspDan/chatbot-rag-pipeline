#!/usr/bin/env python3
"""
Test du processeur de texte avec nos donnÃ©es Service-Public
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.data_extraction.extractors.service_public import ServicePublicExtractor
from src.data_processing.text_processor import TextProcessor

def test_text_processor():
    """Test du chunking intelligent avec vraie donnÃ©e"""
    print("ğŸ§ª TEST PROCESSEUR DE TEXTE")
    print("=" * 50)
    
    # 1. RÃ©cupÃ©rer un document rÃ©el
    print("ğŸ“„ Extraction d'un document test...")
    extractor = ServicePublicExtractor()
    doc = extractor.extract_single_test_fiche()
    
    if not doc:
        print("âŒ Impossible de rÃ©cupÃ©rer un document test")
        return False
    
    print(f"âœ… Document rÃ©cupÃ©rÃ©: {doc.title}")
    print(f"ğŸ“Š Longueur: {len(doc.content)} caractÃ¨res")
    
    # 2. Initialiser le processeur
    processor = TextProcessor(chunk_size=250, overlap=40)
    print("âœ… Processeur initialisÃ©")
    
    # 3. Traiter le document
    print("\nğŸ”„ Chunking du document...")
    chunks = processor.process_document(
        title=doc.title,
        content=doc.content,
        metadata={
            'source_url': doc.source_url,
            'domain': doc.domain,
            'fiche_number': doc.metadata.get('fiche_number')
        }
    )
    
    if not chunks:
        print("âŒ Aucun chunk crÃ©Ã©")
        return False
    
    print(f"âœ… {len(chunks)} chunks crÃ©Ã©s")
    
    # 4. Analyser les chunks
    print("\nğŸ“‹ DÃ‰TAIL DES CHUNKS:")
    print("-" * 60)
    
    for i, chunk in enumerate(chunks):
        print(f"\nğŸ”– Chunk {i+1} ({chunk.chunk_type}):")
        print(f"   ğŸ“ Texte: {chunk.text[:100]}...")
        print(f"   ğŸ“Š Taille: {chunk.char_count} chars, {chunk.word_count} mots")
        print(f"   ğŸ·ï¸ Mots-clÃ©s: {', '.join(chunk.keywords)}")
        
        if chunk.metadata.get('is_title'):
            print(f"   ğŸ‘‘ TITRE PRINCIPAL")
        
        # Afficher quelques mÃ©tadonnÃ©es intÃ©ressantes
        if chunk.metadata.get('has_numbers'):
            print(f"   ğŸ”¢ Contient des chiffres")
        if chunk.metadata.get('complexity') == 'high':
            print(f"   ğŸ§  ComplexitÃ© Ã©levÃ©e")
    
    # 5. Statistiques globales
    stats = processor.get_processing_stats(chunks)
    
    print(f"\nğŸ“Š STATISTIQUES GLOBALES:")
    print("-" * 30)
    print(f"ğŸ”¢ Total chunks: {stats['total_chunks']}")
    print(f"ğŸ“ CaractÃ¨res totaux: {stats['total_characters']:,}")
    print(f"ğŸ“ Mots totaux: {stats['total_words']:,}")
    print(f"ğŸ“ Taille moyenne chunk: {stats['avg_chunk_size']} chars")
    
    print(f"\nğŸ·ï¸ Types de chunks:")
    for chunk_type, count in stats['chunk_types'].items():
        print(f"   - {chunk_type}: {count}")
    
    print(f"\nğŸ“ Distribution tailles:")
    size_dist = stats['size_distribution']
    print(f"   - Petits (<200): {size_dist['small']}")
    print(f"   - Moyens (200-400): {size_dist['medium']}")  
    print(f"   - Grands (>400): {size_dist['large']}")
    
    # 6. Validation qualitÃ©
    print(f"\nğŸ” VALIDATION QUALITÃ‰:")
    print("-" * 25)
    
    # VÃ©rifier les tailles
    oversized = [c for c in chunks if c.char_count > 400]
    undersized = [c for c in chunks if c.char_count < 100]
    
    print(f"âœ… Chunks bien dimensionnÃ©s: {len(chunks) - len(oversized) - len(undersized)}")
    if oversized:
        print(f"âš ï¸ Chunks trop grands: {len(oversized)}")
    if undersized:
        print(f"âš ï¸ Chunks trop petits: {len(undersized)}")
    
    # VÃ©rifier la diversitÃ© des mots-clÃ©s
    all_keywords = set()
    for chunk in chunks:
        all_keywords.update(chunk.keywords)
    
    print(f"ğŸ·ï¸ DiversitÃ© mots-clÃ©s: {len(all_keywords)} catÃ©gories")
    print(f"   CatÃ©gories: {', '.join(sorted(all_keywords))}")
    
    print("\nğŸ‰ TEST CHUNKING TERMINÃ‰ AVEC SUCCÃˆS !")
    return True

if __name__ == "__main__":
    success = test_text_processor()
    sys.exit(0 if success else 1)